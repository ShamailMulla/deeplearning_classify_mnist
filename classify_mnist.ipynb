{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classify_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7kxuRoJEA0t"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==1.15 scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "metadata": {
        "id": "iEk_0I_IHbLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_features_labels(n_labels):\n",
        "  mnist_features, mnist_labels = [], []\n",
        "  mnist = input_data.read_data_sets('/datasets/ud730/mnist',one_hot=True)\n",
        "  for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
        "    # Add features and labels if it's for the first <n>th labels\n",
        "        if mnist_label[:n_labels].any():\n",
        "          mnist_features.append(mnist_feature)\n",
        "          mnist_labels.append(mnist_label)\n",
        "  return mnist_features,mnist_labels"
      ],
      "metadata": {
        "id": "YhtoIXBgVQuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights(n_features,n_labels):\n",
        "  x = tf.Variable(tf.truncated_normal((n_features,n_labels)))\n",
        "  return x"
      ],
      "metadata": {
        "id": "rQ6b7MGjWCG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_biases(n_labels):\n",
        "  x = tf.Variable(tf.zeros(n_labels))\n",
        "  return x"
      ],
      "metadata": {
        "id": "pGbRrAQrWwlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear(input,w,b):\n",
        "  xW = tf.matmul(input,w)\n",
        "  xW_plus_b = tf.add(xW,b)\n",
        "  return xW_plus_b"
      ],
      "metadata": {
        "id": "ZeIrdIo6W_7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 28*28\n",
        "n_labels = 10\n",
        "\n",
        "features = tf.placeholder(tf.float32)\n",
        "labels = tf.placeholder(tf.float32)\n",
        "\n",
        "w = get_weights(n_features,n_labels)\n",
        "b = get_biases(n_labels)\n",
        "\n",
        "logits = linear(features,w,b)\n",
        "\n",
        "train_features, train_labels = mnist_features_labels(n_labels)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  # Softmax\n",
        "  prediction = tf.nn.softmax(logits)\n",
        "\n",
        "  # Cross entropy\n",
        "  # This quantifies how far off the predictions were.\n",
        "  cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
        "\n",
        "  # Training loss\n",
        "  loss = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "  # Rate at which the weights are changed\n",
        "  learning_rate = 0.08\n",
        "\n",
        "  # Gradient Descent\n",
        "  # This is the method used to train the model\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "  # Run optimizer and get loss\n",
        "  _, l = session.run([optimizer, loss],feed_dict={features: train_features, labels: train_labels})\n",
        "\n",
        "print('loss:',l)"
      ],
      "metadata": {
        "id": "q19L4ALYXjkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import getsizeof\n",
        "n_input = 784\n",
        "n_labels = 10\n",
        "mnist = input_data.read_data_sets('datasets/ud730/mnist',one_hot=True)\n",
        "train_features = mnist.train.images\n",
        "val_features = mnist.validation.images\n",
        "train_labels = mnist.train.labels.astype(np.float32)\n",
        "val_labels = mnist.validation.labels.astype(np.float32)\n",
        "test_features = mnist.test.images\n",
        "test_labels = mnist.test.labels.astype(np.float32)\n",
        "\n",
        "weights = tf.Variable(tf.random_normal([n_input,n_labels]))\n",
        "bias = tf.Variable(tf.random_normal([n_labels]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqu-9Dgasboz",
        "outputId": "82a1a4bb-b79a-4087-9e46-965efb84f84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
            "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('shape train features:',train_features.shape)\n",
        "print('bytes features',train_features.nbytes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOSzAliMyat7",
        "outputId": "c6bc87f5-b0c5-4f4a-cc64-c2bb26a572a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape train features: (55000, 784)\n",
            "bytes features 172480000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('shape train labels:',train_labels.shape)\n",
        "print('bytes labels',train_labels.nbytes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8B1mWD7ykGG",
        "outputId": "2abc2927-02a2-4655-c2f4-859b271068c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape train labels: (55000, 10)\n",
            "bytes labels 2200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "m = tf.global_variables_initializer()\n",
        "with tf.Session() as s:\n",
        "  s.run(m)\n",
        "  print('bytes weights',sys.getsizeof(s.run(weights))-112) # sys is adding 112 bytes overhead\n",
        "bias_bytes = n_labels*4 # 4 bytes for each float32 object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10ACqvthyoTY",
        "outputId": "a5b1b72d-56f1-46cb-e146-5cd42b5cdb46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bytes weights 31360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use mini-batching, you must first divide your data into batches.\n",
        "\n",
        "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
        "\n",
        "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's tf.placeholder() function to receive the varying batch sizes.\n",
        "\n",
        "Continuing the example, if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes].\n",
        "\n",
        "What does None do here?\n",
        "The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
        "\n",
        "Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples."
      ],
      "metadata": {
        "id": "VfAkjDuG_I1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = tf.placeholder(tf.float32,[None,n_input])\n",
        "labels = tf.placeholder(tf.float32,[None,n_labels])"
      ],
      "metadata": {
        "id": "v2HqfbNw_DY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "batch_size = 128\n",
        "total_batches = train_features.shape[0]/batch_size\n",
        "last_batch_size = train_features.shape[0]%batch_size\n",
        "print(ceil(total_batches),last_batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nIIlzAi_iUP",
        "outputId": "f0c602b6-5ba0-442f-983c-3df3b1f511c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "430 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batches(batch_size, features, labels):\n",
        "  start, end = 0, batch_size\n",
        "  lst_batches = []\n",
        "  for batch_no in range(ceil(total_batches)):\n",
        "    batch_f = features[start:end]\n",
        "    batch_l = labels[start:end]\n",
        "    lst_batches.append([batch_f,batch_l])\n",
        "    start = end\n",
        "    end += batch_size\n",
        "  # print('Total data points:',len(features))\n",
        "  # print('Start:',start,'End:',end)\n",
        "  return lst_batches"
      ],
      "metadata": {
        "id": "k4ZDhONQA6Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.3\n",
        "epochs = 300\n",
        "result = batches(batch_size,train_features,train_labels)\n",
        "\n",
        "# Logits => xW + b\n",
        "logits = tf.add(tf.matmul(features, weights), bias)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "metadata": {
        "id": "DSGw2j_dCtRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d72724-39d1-4a6a-cc05-7fc13724742f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-a589237a3692>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
        "    \"\"\"\n",
        "    Print cost and validation accuracy of an epoch\n",
        "    \"\"\"\n",
        "    current_cost = sess.run(cost,\n",
        "        feed_dict={features: last_features, labels: last_labels})\n",
        "    valid_accuracy = sess.run(accuracy,\n",
        "        feed_dict={features: val_features, labels: val_labels})\n",
        "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
        "            epoch_i,\n",
        "            current_cost,\n",
        "            valid_accuracy))"
      ],
      "metadata": {
        "id": "ylr-wCdWJ5IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch_i in range(epochs):\n",
        "      lst_batches = batches(batch_size,train_features,train_labels)\n",
        "      # TODO: Train optimizer on all batches\n",
        "      for batch_features, batch_labels in lst_batches:\n",
        "          sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
        "      print_epoch_stats(epoch_i,sess,batch_features,batch_labels)\n",
        "    # Calculate accuracy for test dataset\n",
        "    test_accuracy = sess.run(\n",
        "        accuracy,\n",
        "        feed_dict={features: test_features, labels: test_labels})\n",
        "\n",
        "print('Test Accuracy: {}'.format(test_accuracy)) # 0.92"
      ],
      "metadata": {
        "id": "MRFGJXxfJAnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Network in TensorFlow\n",
        "The focus here is on the architecture of multilayer neural networks, not parameter tuning, so here we'll just give you the learning parameters.\n",
        "The variable n_hidden_layer determines the size of the hidden layer in the neural network. This is also known as the width of a layer.\n",
        "Deep neural networks use multiple layers with each layer requiring it's own weight and bias. The 'hidden_layer' weight and bias is for the hidden layer. The 'out' weight and bias is for the output layer. If the neural network were deeper, there would be weights and biases for each additional layer.\n",
        "\n",
        "**Input**\n",
        "The MNIST data is made up of 28px by 28px images with a single channel. The tf.reshape() function above reshapes the 28px by 28px matrices in x into row vectors of 784px."
      ],
      "metadata": {
        "id": "ydANiFl_wwBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)"
      ],
      "metadata": {
        "id": "RuPj8MSJz5Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout\n",
        "Dropout is a regularization technique for reducing overfitting. The technique temporarily drops units (artificial neurons) from the network, along with all of those units' incoming and outgoing connections. Figure 1 illustrates how dropout works.\n",
        "\n",
        "You should only drop units while training the model. During validation or testing, you should keep all of the units to maximize accuracy."
      ],
      "metadata": {
        "id": "ZC7KY5oP43dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 30\n",
        "hidden_layer_size = 256\n",
        "batch_size = 128\n",
        "display_step = 2\n",
        "n_input = 784  # MNIST data input (img shape: 28*28)\n",
        "n_labels = 10  # MNIST total classes (0-9 digits)"
      ],
      "metadata": {
        "id": "eg32JhNswvA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = {\n",
        "    'hidden_layer':tf.Variable(tf.random_normal([n_input,hidden_layer_size]),name='weights_hidden'),\n",
        "    'out':tf.Variable(tf.random_normal([hidden_layer_size,n_labels]),name='weights_out')\n",
        "}\n",
        "biases = {\n",
        "    'hidden_layer':tf.Variable(tf.random_normal([hidden_layer_size]),name='bias_hidden'),\n",
        "    'out':tf.Variable(tf.random_normal([n_labels]),name='bias_out')\n",
        "}"
      ],
      "metadata": {
        "id": "r9uYuEbJHZH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, 28, 28, 1],name='features')\n",
        "y = tf.placeholder(tf.float32, [None, n_labels],name='labels')\n",
        "\n",
        "x_flat = tf.reshape(x, [-1, n_input])\n",
        "keep_prob = tf.placeholder(tf.float32,name='keep_prob')"
      ],
      "metadata": {
        "id": "aJjechKdxYZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hidden layer with RELU activation\n",
        "hidden_layer = tf.add(tf.matmul(x_flat,weights['hidden_layer']),biases['hidden_layer'])\n",
        "hidden_layer = tf.nn.relu(hidden_layer)\n",
        "hidden_layer = tf.nn.dropout(hidden_layer,keep_prob)\n",
        "# Output layer with linear activation\n",
        "logits = tf.add(tf.matmul(hidden_layer,weights['out']),biases['out'])"
      ],
      "metadata": {
        "id": "wvRwDyYOxilO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3795638c-c47a-44f3-c739-ef0568bbca9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-7241c4c22a20>:4: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimiser\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y))\n",
        "optimiser = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "# Calculate accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
      ],
      "metadata": {
        "id": "kcfDwIdryYUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34925e56-3e00-49a6-965c-a21e3ef732cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-61531909fe8c>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Variables\n",
        "Training a model can take hours. But once you close your TensorFlow session, you lose all the trained weights and biases. If you were to reuse the model in the future, you would have to train it all over again!\n",
        "If you're using TensorFlow 0.11.0RC1 or newer, a file called \"model.ckpt.meta\" will also be created. This file contains the TensorFlow graph."
      ],
      "metadata": {
        "id": "JJnyHWV200yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_file = './model.ckpt'\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as s:\n",
        "  s.run(init)\n",
        "  for epoch_i in range(training_epochs):\n",
        "    total_batches = int(mnist.train.num_examples/batch_size)\n",
        "    for i in range(total_batches):\n",
        "      batch_x, batch_y = mnist.train.next_batch(batch_size) # automatically creating batches\n",
        "      s.run(optimiser,feed_dict={x:batch_x,y:batch_y,keep_prob:0.5})\n",
        "    if epoch_i%6==0:\n",
        "      validation_accuracy = s.run(accuracy,feed_dict={x:mnist.validation.images,y:mnist.validation.labels,keep_prob:1})\n",
        "      print('Epoch {:<3} - Validation Accuracy: {}'.format(\n",
        "                epoch_i,validation_accuracy))\n",
        "  test_accuracy = s.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1})\n",
        "  print('Test accuracy',test_accuracy)\n",
        "  saver.save(s,save_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLIJj4-My5Mq",
        "outputId": "853041b7-00b7-438a-ef74-dde9f1f1c9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0   - Validation Accuracy: 0.42480000853538513\n",
            "Epoch 6   - Validation Accuracy: 0.7757999897003174\n",
            "Epoch 12  - Validation Accuracy: 0.8284000158309937\n",
            "Epoch 18  - Validation Accuracy: 0.8492000102996826\n",
            "Epoch 24  - Validation Accuracy: 0.8593999743461609\n",
            "Test accuracy 0.866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Variables\n",
        "Now that the Tensor Variables are saved, let's load them back into a new model.\n",
        "\n",
        "You'll notice you still need to create the weights and bias Tensors in Python. The tf.train.Saver.restore() function loads the saved data into weights and bias.\n",
        "\n",
        "Since tf.train.Saver.restore() sets all the TensorFlow Variables, you don't need to call tf.global_variables_initializer()."
      ],
      "metadata": {
        "id": "vO7-oHDQ1izp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "# Add all the parameters to be used again\n",
        "x = tf.placeholder(tf.float32,[None,n_input])\n",
        "x_flat = tf.reshape(x, [-1, n_input])\n",
        "y = tf.placeholder(tf.float32,[None,n_labels])\n",
        "keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
        "weights = {\n",
        "    'hidden_layer':tf.Variable(tf.random_normal([n_input,hidden_layer_size]),name='weights_hidden'),\n",
        "    'out':tf.Variable(tf.random_normal([hidden_layer_size,n_labels]),name='weights_out')\n",
        "}\n",
        "biases = {\n",
        "    'hidden_layer':tf.Variable(tf.random_normal([hidden_layer_size]),name='bias_hidden'),\n",
        "    'out':tf.Variable(tf.random_normal([n_labels]),name='bias_out')\n",
        "}\n",
        "# Hidden layer with RELU activation\n",
        "hidden_layer = tf.add(tf.matmul(x_flat,weights['hidden_layer']),biases['hidden_layer'])\n",
        "hidden_layer = tf.nn.relu(hidden_layer)\n",
        "hidden_layer = tf.nn.dropout(hidden_layer,keep_prob)\n",
        "# Output layer with linear activation\n",
        "logits = tf.add(tf.matmul(hidden_layer,weights['out']),biases['out'])\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSOp9sr30z4D",
        "outputId": "3c248a8e-e6fc-447a-8203-a0162c2c64ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
            "Weights:\thidden_layer: (784, 256) output_layer: (256, 10)\n",
            "Biases\thidden_layer: (256,) output_layer: (10,)\n",
            "0.866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restore all the values in the params just created & run to find test accuracy\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as s:\n",
        "  saver.restore(s,save_file)\n",
        "  weights = s.run(weights)\n",
        "  biases = s.run(biases)\n",
        "  print('Weights:\\thidden_layer:',weights['hidden_layer'].shape,'output_layer:',weights['out'].shape)\n",
        "  print('Biases\\thidden_layer:',biases['hidden_layer'].shape,'output_layer:',biases['out'].shape)\n",
        "  \n",
        "  test_accuracy = s.run(accuracy,feed_dict={\n",
        "      x:mnist.test.images.reshape(-1, n_input),\n",
        "      y:mnist.test.labels,\n",
        "      keep_prob:1.0\n",
        "      })\n",
        "  print(test_accuracy)"
      ],
      "metadata": {
        "id": "ObAOAnzQWPr8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}